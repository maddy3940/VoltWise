{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from prophet import Prophet\n",
    "import numpy as np\n",
    "import pickle\n",
    "import boto3\n",
    "import io\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    directory= \"/workspace/VoltWise/Data_Ingestion/daily_data/\"\n",
    "\n",
    "    # Read from an S3 bucket\n",
    "    bucket_name = 'tempbucketvoltwise'\n",
    "    key = 'CAL.csv'\n",
    "\n",
    "    region_data = {}\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.parquet'):\n",
    "            # Extract the region name from the file name\n",
    "            region = filename.split('.')[0]\n",
    "\n",
    "            # Read the contents of the file into a variable\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            df = pd.read_parquet(filepath)\n",
    "            \n",
    "            # Store the data in the dictionary with the region name as the key\n",
    "            region_data[region] = df\n",
    "    \n",
    "    return region_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the access keys\n",
    "access_key_id = ''\n",
    "secret_access_key = ''\n",
    "\n",
    "# Create an S3 client\n",
    "s3_client = boto3.client('s3', aws_access_key_id=access_key_id, aws_secret_access_key=secret_access_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read\n",
    "response = s3_client.get_object(Bucket=bucket_name, Key=key)\n",
    "data = response['Body'].read()\n",
    "df = io.BytesIO(data)\n",
    "df = pd.read_csv(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '3KZR52Y03YDWT0WW',\n",
       "  'HostId': '1jFlKqvUS8+s2mX2Rsw/au4qZ1XMvrDhqZ4wbeU76wClYpONP+eukWnUO9+Rbt/CXtS8gLYk1tw=',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amz-id-2': '1jFlKqvUS8+s2mX2Rsw/au4qZ1XMvrDhqZ4wbeU76wClYpONP+eukWnUO9+Rbt/CXtS8gLYk1tw=',\n",
       "   'x-amz-request-id': '3KZR52Y03YDWT0WW',\n",
       "   'date': 'Thu, 29 Jun 2023 08:24:25 GMT',\n",
       "   'x-amz-server-side-encryption': 'AES256',\n",
       "   'etag': '\"16ff39010b52de7b695c8e7f9e9235fe\"',\n",
       "   'server': 'AmazonS3',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 0},\n",
       " 'ETag': '\"16ff39010b52de7b695c8e7f9e9235fe\"',\n",
       " 'ServerSideEncryption': 'AES256'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write\n",
    "output_data = df.to_csv(index=False)\n",
    "\n",
    "# Convert the CSV data to bytes\n",
    "output_bytes = output_data.encode('utf-8')\n",
    "\n",
    "# Write the CSV data to the bucket\n",
    "s3_client.put_object(Body=output_bytes, Bucket=bucket_name, Key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_data= load_data()            \n",
    "regions = list(region_data.keys())\n",
    "data_types = ['demand', 'generation']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
