{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/VoltWise/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from prophet import Prophet\n",
    "import numpy as np\n",
    "import pickle\n",
    "import boto3\n",
    "import io\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Read from an S3 bucket\n",
    "    bucket_name = 'ecc-eia-data'\n",
    "    key = 'raw_data_daily/'\n",
    "\n",
    "    response = s3_client.list_objects_v2(Bucket=bucket_name,Prefix=key)\n",
    "    \n",
    "    # Extract the file names\n",
    "    file_names = [os.path.basename(obj['Key']) for obj in response['Contents']]\n",
    "\n",
    "    region_data = {}\n",
    "    for filename in file_names:\n",
    "        if filename.endswith('.csv'):\n",
    "            # Extract the region name from the file name\n",
    "            region = filename.split('.')[0]\n",
    "            df = read_from_s3(bucket_name,key+f'{region}.csv')\n",
    "            # Store the data in the dictionary with the region name as the key\n",
    "            region_data[region] = df\n",
    "            #write_to_s3(df,'models-prophet','temp/',f'{region}.csv')\n",
    "\n",
    "    return region_data\n",
    "\n",
    "def preprocess_data(data, region):\n",
    "    # Filter data for the specified region\n",
    "    region_data = data[region]\n",
    "\n",
    "    # region_data['Date'] = pd.to_datetime(region_data['Date'], format='%Y-%m-%d')\n",
    "    region_data['Timestamp'] = region_data['Date']\n",
    "\n",
    "    # Prepare separate datasets for demand and net generation\n",
    "    demand_data = region_data[['Timestamp', 'Demand']]\n",
    "    demand_data.columns = ['ds', 'y']\n",
    "\n",
    "    # Drop rows with missing values\n",
    "    demand_data = demand_data.dropna()\n",
    "\n",
    "    generation_data = region_data[['Timestamp', 'Net generation']]\n",
    "    generation_data.columns = ['ds', 'y']\n",
    "\n",
    "    # Drop rows with missing values\n",
    "    generation_data = generation_data.dropna()\n",
    "\n",
    "    return demand_data, generation_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read function\n",
    "def read_from_s3(bucket_name,key):\n",
    "    response = s3_client.get_object(Bucket=bucket_name, Key=key)\n",
    "    data = response['Body'].read()\n",
    "    df = io.BytesIO(data)\n",
    "    df = pd.read_csv(df)\n",
    "    return df\n",
    "\n",
    "def write_to_s3(df,bucket_name,key,filename):\n",
    "    output_data = df.to_csv(index=False)\n",
    "    # Convert the CSV data to bytes\n",
    "    output_bytes = output_data.encode('utf-8')\n",
    "    # Write the CSV data to the bucket\n",
    "    s3_client.put_object(Body=output_bytes, Bucket=bucket_name, Key=key+filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_prophet_model(region, data_type, data):\n",
    "    \"\"\"\n",
    "    Train a Prophet model for a given region and data type (demand or net generation).\n",
    "\n",
    "    Args:\n",
    "    region (str): The region code.\n",
    "    data_type (str): 'Demand' or 'Net generation'.\n",
    "    data (pd.DataFrame): The preprocessed data for the region, with columns 'ds' and 'y'.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Create and fit the Prophet model\n",
    "    model = Prophet()\n",
    "    model.fit(data)\n",
    "    model_bytes  = pickle.dumps(model)\n",
    "\n",
    "    # Specify the bucket name and object key\n",
    "    bucket_name = 'models-prophet'\n",
    "    object_key = f'temp/{region}_{data_type}_model.pkl'\n",
    "\n",
    "    # Write the pickle file to S3 bucket\n",
    "    s3_client.put_object(Body=model_bytes, Bucket=bucket_name, Key=object_key)\n",
    "\n",
    "    # Print the confirmation message\n",
    "    print(f\"Prophet model for {region} {data_type} has been stored in S3 bucket.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the access keys\n",
    "access_key_id = 'AKIAZIMSUAOJMLAWL5SF'\n",
    "secret_access_key = '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'\n",
    "\n",
    "# Create an S3 client\n",
    "s3_client = boto3.client('s3', aws_access_key_id=access_key_id, aws_secret_access_key=secret_access_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_data= load_data()            \n",
    "regions = list(region_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data for each region and store it in a dictionary\n",
    "preprocessed_data = {}\n",
    "for region in regions:\n",
    "    demand_data, generation_data = preprocess_data(region_data.copy(), region)\n",
    "    preprocessed_data[region] = {'demand': demand_data, 'generation': generation_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:34:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:34:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:34:25 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prophet model for CAL demand has been stored in S3 bucket.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:34:25 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:34:26 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prophet model for CAL generation has been stored in S3 bucket.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:34:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:34:26 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prophet model for CAR demand has been stored in S3 bucket.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:34:26 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:34:27 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prophet model for CAR generation has been stored in S3 bucket.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:34:27 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:34:27 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prophet model for CENT demand has been stored in S3 bucket.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:34:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:34:28 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prophet model for CENT generation has been stored in S3 bucket.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:34:28 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:34:29 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prophet model for FLA demand has been stored in S3 bucket.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:34:29 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:34:29 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prophet model for FLA generation has been stored in S3 bucket.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:34:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:34:30 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prophet model for MIDA demand has been stored in S3 bucket.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:34:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:34:30 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prophet model for MIDA generation has been stored in S3 bucket.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:34:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:34:31 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prophet model for MIDW demand has been stored in S3 bucket.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:34:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:34:32 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prophet model for MIDW generation has been stored in S3 bucket.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:34:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:34:32 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prophet model for NE demand has been stored in S3 bucket.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:34:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:34:33 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prophet model for NE generation has been stored in S3 bucket.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:34:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:34:33 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prophet model for NY demand has been stored in S3 bucket.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:34:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:34:34 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prophet model for NY generation has been stored in S3 bucket.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:34:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:34:34 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prophet model for SE demand has been stored in S3 bucket.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:34:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:34:35 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prophet model for SE generation has been stored in S3 bucket.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:34:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:34:36 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prophet model for SW demand has been stored in S3 bucket.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:34:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:34:37 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prophet model for SW generation has been stored in S3 bucket.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:34:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "16:34:37 - cmdstanpy - INFO - Chain [1] start processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prophet model for TEX demand has been stored in S3 bucket.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16:34:37 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prophet model for TEX generation has been stored in S3 bucket.\n"
     ]
    }
   ],
   "source": [
    "data_types = ['demand', 'generation']\n",
    "\n",
    "for region in regions:\n",
    "    for data_type in data_types:\n",
    "        region_preprocessed_data = preprocessed_data[region][data_type]\n",
    "        train_prophet_model(region, data_type, region_preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
